{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip -q install -U datasets transformers accelerate evaluate scikit-learn\n!pip -q install -U evaluate\nimport os, zipfile, numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    DataCollatorWithPadding, TrainingArguments, Trainer\n)\nimport evaluate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:34:15.448527Z","iopub.execute_input":"2026-01-08T15:34:15.448759Z","iopub.status.idle":"2026-01-08T15:34:18.897418Z","shell.execute_reply.started":"2026-01-08T15:34:15.448728Z","shell.execute_reply":"2026-01-08T15:34:18.896570Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"ds = load_dataset(\"ailsntua/QEvasion\")  # train: 3448, test: 308\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:34:26.854381Z","iopub.execute_input":"2026-01-08T15:34:26.855231Z","iopub.status.idle":"2026-01-08T15:34:29.908408Z","shell.execute_reply.started":"2026-01-08T15:34:26.855197Z","shell.execute_reply":"2026-01-08T15:34:29.907886Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Tạo validation từ train để theo dõi macro-F1\nsplits = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\ntrain_ds = splits[\"train\"]\nval_ds   = splits[\"test\"]\ntest_ds  = ds[\"test\"]\n\n# Nhãn Task 1\nlabel_list = [\"Ambivalent\", \"Clear Non-Reply\", \"Clear Reply\"]\nlabel2id = {l:i for i,l in enumerate(label_list)}\nid2label = {i:l for l,i in label2id.items()}\n\ndef encode_label(example):\n    example[\"labels\"] = label2id[example[\"clarity_label\"]]\n    return example\n\ntrain_ds = train_ds.map(encode_label)\nval_ds   = val_ds.map(encode_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:34:33.737557Z","iopub.execute_input":"2026-01-08T15:34:33.738319Z","iopub.status.idle":"2026-01-08T15:34:33.760449Z","shell.execute_reply.started":"2026-01-08T15:34:33.738293Z","shell.execute_reply":"2026-01-08T15:34:33.759937Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\nmodel_name = \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nQ_COL = \"question\"\nA_COL = \"interview_answer\"\n\ndef preprocess(batch):\n    # Tokenize sequence pair: (text, text_pair)\n    return tokenizer(\n        batch[Q_COL],\n        batch[A_COL],\n        truncation=True,\n        max_length=512,\n    )\n\ntrain_tok = train_ds.map(preprocess, batched=True)\nval_tok   = val_ds.map(preprocess, batched=True)\n\n# Giữ cột cần cho Trainer: input_ids/attention_mask/labels\nkeep = {\"input_ids\", \"attention_mask\", \"labels\"}\ntrain_tok = train_tok.remove_columns([c for c in train_tok.column_names if c not in keep])\nval_tok   = val_tok.remove_columns([c for c in val_tok.column_names if c not in keep])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:35:48.445063Z","iopub.execute_input":"2026-01-08T15:35:48.445796Z","iopub.status.idle":"2026-01-08T15:35:52.813435Z","shell.execute_reply.started":"2026-01-08T15:35:48.445772Z","shell.execute_reply":"2026-01-08T15:35:52.812874Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3103 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959a1da77c2f40b0ae604844d02a76aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e5a57f12eeb4fae9d61d3b2e08bc265"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport evaluate\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n# metric (macro F1)\nf1 = evaluate.load(\"f1\")  # load once (đúng best practice) [web:84]\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id,\n)\n\nargs = TrainingArguments(\n    output_dir=\"xlmr_clarity_baseline\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",     # dùng key chuẩn/ổn định [web:71]\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"macro_f1\",  # Trainer chấp nhận tên metric trả về (có/không \"eval_\" prefix) [web:118]\n    greater_is_better=True,\n    logging_steps=50,\n    fp16=True,                        # bật nếu có CUDA; nếu máy bạn không có GPU thì set False [web:126]\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:36:41.585010Z","iopub.execute_input":"2026-01-08T15:36:41.586011Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_186/4280817983.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='232' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [232/582 03:22 < 05:07, 1.14 it/s, Epoch 1.19/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.909200</td>\n      <td>0.843346</td>\n      <td>0.255952</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# tokenize test (không có labels)\ntest_tok = test_ds.map(preprocess, batched=True, remove_columns=test_ds.column_names)\n\npred = trainer.predict(test_tok)\npred_ids = np.argmax(pred.predictions, axis=-1)\npred_labels = [id2label[i] for i in pred_ids]\n\n# ghi file extensionless: \"prediction\"\nwith open(\"prediction\", \"w\", encoding=\"utf-8\") as f:\n    for lab in pred_labels:\n        f.write(lab + \"\\n\")\n\nprint(\"Wrote:\", os.path.abspath(\"prediction\"))\nprint(\"Num lines:\", sum(1 for _ in open(\"prediction\", \"r\", encoding=\"utf-8\")))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = \"submission.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    z.write(\"prediction\", arcname=\"prediction\")  # quan trọng: arcname không có folder\n\nprint(\"Zipped:\", os.path.abspath(zip_path))\n\n# sanity check: xem zip có đúng 1 file prediction không\nwith zipfile.ZipFile(zip_path, \"r\") as z:\n    print(\"ZIP content:\", z.namelist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}